{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6G8F2UkAzUUy+LfhVR28h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Partha-Guntur/News-Analysis-Web-Scrapping/blob/main/web_scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dowload the Required Modules"
      ],
      "metadata": {
        "id": "0egmh06tqZm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imzxxl61dxsx",
        "outputId": "522419d6-843b-45e1-81ea-756bd7af73f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjV0uAZKd4nJ",
        "outputId": "795b6edd-7c19-4fdc-887b-2184f5bc1ef3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtfZa0W-eFzW",
        "outputId": "cbe9ac51-0b51-44f6-d079-35396ccbb2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.4)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open the Hyperlink"
      ],
      "metadata": {
        "id": "wt5qOrhAqefO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl\n",
        "\n",
        "workbook = openpyxl.load_workbook('/content/drive/MyDrive/20211030 Test Assignment/Input.xlsx')\n",
        "sheet = workbook.active\n",
        "\n",
        "hyperlinks = []\n",
        "\n",
        "for row in sheet.iter_rows():\n",
        "    for cell in row:\n",
        "        if cell.hyperlink:\n",
        "            hyperlinks.append(cell.hyperlink.target)\n"
      ],
      "metadata": {
        "id": "khKImQHsgEHP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Titles"
      ],
      "metadata": {
        "id": "N3WqzIguqjEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "article_titles = []\n",
        "\n",
        "for link in hyperlinks:\n",
        "    response = requests.get(link)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    title = soup.find('h1').text\n",
        "    article_titles.append(title)\n",
        "\n",
        "# URL_ID save it in a text file\n",
        "\n",
        "with open('URL_ID.txt', 'w') as file:\n",
        "    for title in article_titles:\n",
        "        file.write(title + '\\n')\n"
      ],
      "metadata": {
        "id": "D3Tvw7aKgN06"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing Output in the Excel File"
      ],
      "metadata": {
        "id": "NcYGicEpqUOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headTexts = []\n",
        "with open('URL_ID.txt', 'r') as file:\n",
        "    headTexts = file.readlines()\n",
        "    file.read()"
      ],
      "metadata": {
        "id": "oT0WeGdHhQ1m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using NLP techniques to perform textual analysis"
      ],
      "metadata": {
        "id": "J0dayhGLsWEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAb1ePmcsLUx",
        "outputId": "c11beba9-18f5-4304-aac6-9ff3b041482f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Stopwords"
      ],
      "metadata": {
        "id": "wB70nOlNxdjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "title_list = []\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    headTexts = text\n",
        "    words = []\n",
        "    for sentence in headTexts:\n",
        "        tokens = nltk.word_tokenize(sentence)\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum()]\n",
        "        tokens = [word for word in tokens if word not in stopwords]\n",
        "        words.append(tokens)\n",
        "\n",
        "    return words\n",
        "\n",
        "title_list = remove_stopwords(headTexts)\n"
      ],
      "metadata": {
        "id": "LMqc9Pzrs1yT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('opinion_lexicon')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA6FUDxoAOig",
        "outputId": "f7293504-b11f-47db-8d9b-8a53a0e028db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "joined_sentences = [' '.join(sentence) for sentence in title_list]\n",
        "polarity_scores = []\n",
        "\n",
        "for sentence in joined_sentences:\n",
        "    scores = sid.polarity_scores(sentence)\n",
        "    compound = scores['compound']\n",
        "    if compound >= 0.05:\n",
        "        sentiment = \"Positive\"\n",
        "    elif compound <= -0.05:\n",
        "        sentiment = \"Negative\"\n",
        "        # print(f\"Sentence: {sentence}\\nSentiment: {sentiment}, Polarity Score: {scores}, Sentence Length: {len(sentence)}\\n\")\n",
        "    polarity_scores.append(scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNTBiDD84uxx",
        "outputId": "535278ca-69ce-4b77-9160-e414429e762c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the 'polarity' value\n",
        "polarity_values = [score['compound'] for score in polarity_scores[:-1]]\n",
        "pos_values = [score['pos'] for score in polarity_scores[:-1]]\n",
        "neg_values = [score['neg'] for score in polarity_scores[:-1]]"
      ],
      "metadata": {
        "id": "yfQy9N-gTA1q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openpyxl import load_workbook\n",
        "\n",
        "workbook = load_workbook('/content/drive/MyDrive/20211030 Test Assignment/Output Data Structure.xlsx')\n",
        "sheet = workbook.active\n",
        "\n",
        "header = [cell.value for cell in sheet[1]]\n",
        "polarity_col_index = header.index('POSITIVE SCORE') + 1\n",
        "\n",
        "for i, score in enumerate(pos_values, start=2):\n",
        "    sheet.cell(row=i, column=polarity_col_index, value=score)\n",
        "\n",
        "# Save the workbook\n",
        "workbook.save('Output Data Structure.xlsx')"
      ],
      "metadata": {
        "id": "vT3DVjEBYz3h"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openpyxl import load_workbook\n",
        "\n",
        "workbook = load_workbook('Output Data Structure.xlsx')\n",
        "sheet = workbook.active\n",
        "\n",
        "header = [cell.value for cell in sheet[1]]\n",
        "polarity_col_index = header.index('NEGATIVE SCORE') + 1\n",
        "\n",
        "for i, score in enumerate(neg_values, start=2):\n",
        "    sheet.cell(row=i, column=polarity_col_index, value=score)\n",
        "\n",
        "workbook.save('Output Data Structure.xlsx')"
      ],
      "metadata": {
        "id": "TgMvn1mEY4Gz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openpyxl import load_workbook\n",
        "\n",
        "workbook = load_workbook('Output Data Structure.xlsx')\n",
        "sheet = workbook.active\n",
        "\n",
        "header = [cell.value for cell in sheet[1]]\n",
        "polarity_col_index = header.index('POLARITY SCORE') + 1\n",
        "\n",
        "for i, score in enumerate(polarity_values, start=2):\n",
        "    sheet.cell(row=i, column=polarity_col_index, value=score)\n",
        "\n",
        "workbook.save('Output Data Structure.xlsx')"
      ],
      "metadata": {
        "id": "Kxl4sRlbfqDp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insertion_values = ['POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']"
      ],
      "metadata": {
        "id": "t99lNbHpkONM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openpyxl import load_workbook\n",
        "\n",
        "avg_senteces_length = []\n",
        "for word in title_list:\n",
        "    avg_senteces_length.append(len(word)/len(title_list))\n",
        "\n",
        "workbook = load_workbook('Output Data Structure.xlsx')\n",
        "sheet = workbook.active\n",
        "\n",
        "header = [cell.value for cell in sheet[1]]\n",
        "polarity_col_index = header.index('AVG SENTENCE LENGTH') + 1\n",
        "\n",
        "for i, score in enumerate(avg_senteces_length, start=2):\n",
        "    sheet.cell(row=i, column=polarity_col_index, value=score)\n",
        "\n",
        "workbook.save('Output Data Structure.xlsx')"
      ],
      "metadata": {
        "id": "yILVtE_9ivns"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
        "avg_senteces_length = []\n",
        "complex_words = []\n",
        "\n",
        "for word in title_list:\n",
        "    avg_senteces_length.append(len(word)/len(title_list))\n",
        "for title in title_list:\n",
        "  for word in title:\n",
        "    if word.islower():\n",
        "      complex_words.append(word)\n"
      ],
      "metadata": {
        "id": "TuCOnHdVdbRp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('cmudict')\n"
      ],
      "metadata": {
        "id": "6gTrBWz2jRDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2950851-cf3c-44ce-f81f-562d0d51ec2f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.corpus import cmudict\n",
        "\n",
        "d = cmudict.dict()\n",
        "\n",
        "def count_syllables(word):\n",
        "    if word.lower() in d:\n",
        "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def is_complex_word(word, syllable_threshold=3, length_threshold=7):\n",
        "    syllables = count_syllables(word)\n",
        "    return syllables >= syllable_threshold or len(word) >= length_threshold\n",
        "\n",
        "def find_complex_words(sentence, syllable_threshold=3, length_threshold=7):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    complex_words = [word for word in words if is_complex_word(word, syllable_threshold, length_threshold)]\n",
        "    return complex_words\n",
        "\n",
        "complex_words = []\n",
        "for i in range(len(joined_sentences)):\n",
        "  sentence = joined_sentences[i]\n",
        "  complex_words.append(find_complex_words(sentence))\n",
        "\n",
        "complex_words"
      ],
      "metadata": {
        "id": "AMU-C47-wRJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8879d64b-d846-49cc-adc9-c2d3e882a418"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['economy', 'environment', 'infrastructure'],\n",
              " ['Economy', 'Environment', 'Infrastructure'],\n",
              " ['Internet', 'Evolution', 'Communication', 'Alternative', 'Pathways'],\n",
              " ['Cybercrime', 'upcoming'],\n",
              " ['platform', 'entertainment', 'industry'],\n",
              " ['platform', 'entertainment', 'industry'],\n",
              " ['Effects'],\n",
              " ['Internet', 'Communications', 'Alternatives'],\n",
              " ['Cybercrime'],\n",
              " ['Cybercrime'],\n",
              " ['Internet', 'Communications', 'Alternatives'],\n",
              " ['telemedicine', 'Livelihood'],\n",
              " [],\n",
              " [],\n",
              " ['telemedicine', 'Livelihood'],\n",
              " ['telemedicine', 'Livelihood'],\n",
              " ['Chatbots', 'customer', 'support'],\n",
              " [],\n",
              " ['marketing', 'influence', 'business', 'consumer'],\n",
              " ['advertisement', 'increase'],\n",
              " ['Negative', 'marketing', 'society'],\n",
              " ['business'],\n",
              " ['economy', 'environment', 'infrastructure'],\n",
              " ['platform', 'entertainment', 'industry'],\n",
              " ['Electric', 'Vehicles', 'Livelihood'],\n",
              " ['electric', 'vehicle', 'livelihood'],\n",
              " ['economy'],\n",
              " ['outlook', 'healthcare'],\n",
              " ['healthcare', 'Improve', 'Patient', 'Outcomes'],\n",
              " ['Creation', 'Creator'],\n",
              " [],\n",
              " ['Impacts', 'product'],\n",
              " ['Impacts', 'Vegetable', 'Vendors'],\n",
              " ['pandemic', 'Tourism', 'Aviation', 'industry'],\n",
              " ['pandemic'],\n",
              " ['Changing', 'landscape', 'emerging', 'Indian', 'Industry'],\n",
              " ['Adolescent',\n",
              "  'demotivated',\n",
              "  'depression',\n",
              "  'musculoskeletal',\n",
              "  'psychosomatic',\n",
              "  'symptom'],\n",
              " ['Outlook'],\n",
              " ['business', 'successful', 'business'],\n",
              " ['redefining', 'service'],\n",
              " ['increase', 'medium', 'engagement', 'marketer'],\n",
              " ['Impacts', 'Streets'],\n",
              " ['Coronavirus', 'energy'],\n",
              " ['Hospitality', 'Industry'],\n",
              " ['Lessons', 'learning', 'relevant', 'coronavirus'],\n",
              " ['Estimating'],\n",
              " ['Estimating'],\n",
              " ['Tourism', 'Outlook'],\n",
              " ['Disorder', 'Effects'],\n",
              " ['repercussion', 'environment', 'pandemic', 'situation'],\n",
              " ['repercussion', 'environment'],\n",
              " ['pandemic', 'industry'],\n",
              " ['Contribution', 'handicraft', 'Visual', 'Literature', 'Indian', 'economy'],\n",
              " ['impacting', 'payment', 'preference'],\n",
              " []]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pos_values), len(neg_values), len(joined_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL9F-XjE6I3Z",
        "outputId": "7167e3ae-0760-4170-de10-5d9e8e6078ed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(54, 54, 55)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1 = []\n",
        "for i in range(len(joined_sentences)-1):\n",
        "  list1.append(float(((pos_values[i] + neg_values[i])/len(joined_sentences[i]) + 0.000001)))\n",
        "\n",
        "subj_score = []\n",
        "for i in range(len(list1)):\n",
        "  subj_score.append(f\"{list1[i]:.10f}\")"
      ],
      "metadata": {
        "id": "6lAkLfo4wV3v"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workbook = load_workbook('Output Data Structure.xlsx')\n",
        "sheet = workbook.active\n",
        "\n",
        "header = [cell.value for cell in sheet[1]]\n",
        "polarity_col_index = header.index('SUBJECTIVITY SCORE') + 1\n",
        "\n",
        "for i, score in enumerate(subj_score, start=2):\n",
        "    sheet.cell(row=i, column=polarity_col_index, value=score)\n",
        "\n",
        "workbook.save('Output Data Structure.xlsx')"
      ],
      "metadata": {
        "id": "rPBjEng38CDx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sheet = workbook['Sheet1']\n",
        "sheet.delete_cols(8, 1)\n",
        "sheet.delete_cols(11, 1)\n",
        "sheet.delete_cols(12, 1)\n",
        "\n",
        "workbook.save('Output Data Structure.xlsx')\n"
      ],
      "metadata": {
        "id": "gAXNx6Zu8RFH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gSDFdf-Fhd3f"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}